{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Q-Network\n",
    "Acknowledgement: https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df\n",
    "\n",
    "17/02/2017\n",
    "\n",
    "From Q network to deep Q network.\n",
    "More improvement has been made to Deep Q-network. Now double DQN and Dueling DQN improve performance, staility and faster training time.\n",
    "\n",
    "Revision: Convolutional layers consider a region of image instead of looking at each pixel indipendently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technical:\n",
    "Convolution layer premade function in TF: _tf.contrib.layers.convolution2d_\n",
    "```\n",
    "convolution_layer = tf.contrib.layers.convolution2d(inputs,num_outputs,kernel_size,stride,padding)\n",
    "```\n",
    "- num_outs - filters to the previous layer?\n",
    "- kernal_size - sliding window size, to slide over the previous layer\n",
    "- stride - how many pixel to skip when sliding window across layer\n",
    "- padding - pad window to the same dimension as the previous layer or not\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experience replay - stores agent experiences and then randomly drawing batches of them to train the network.\n",
    "\n",
    "Experiences are stored as a _tuple_.\n",
    "``` <state, action, reward, next state>```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DDQN__\n",
    "- Instead of taking the max over Q-values when computing the target-Q value\n",
    "- Use primary netowrk to chose an action\n",
    "- Use target network to generate the target Q-value for that action\n",
    "\n",
    "__Why?__\n",
    "- Cause suboptimal actinons regularly gives higher Q-values than optimal actions\n",
    "\n",
    "DDQN equation for updating the target value:\n",
    "\n",
    " _```Q-Target = r + γQ(s’,argmax(Q(s’,a,ϴ),ϴ’))```_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dueling Dqn__\n",
    "- Goal of Dueling DQN is to have a network that separately computes the advantage and value function, and combines them back into a single Q-function only at the final layer.\n",
    "\n",
    "__```Q(s,a)```__ correspond to - how good it is to take _certain action_ given a _certain state_.\n",
    "This notion can be broken down into: ```V(s) and A(a)```\n",
    "- Value function V(s) - the value of the given state \n",
    "- Advantage function A(a) - how much better taking a certain action would be compared to others\n",
    "\n",
    "Therefore:\n",
    "\n",
    "_```Q(s,a) = V(s) + A(a)```_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Notes: I'm not going to try the code since my computer is not powerful and I cannot install Tensorflow on the lab comps. I am going to swap over to the lab computer now and try pytorch._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
